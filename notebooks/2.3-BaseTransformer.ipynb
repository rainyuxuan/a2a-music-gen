{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-13T00:53:24.285192Z",
     "start_time": "2024-04-13T00:53:20.197532Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchaudio.functional\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, Tensor\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "\n",
    "from src.data import WaveDataset, SpectrogramDataset\n",
    "from src.visualization import plot_spectrogram, plot_waveform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wdir = os.getcwd().replace(\"\\\\\", \"/\") + \"/..\"\n",
    "processed_dir = f\"{wdir}/data/processed/musicnet\"\n",
    "output_dir = f\"{wdir}/data/output/musicnet\"\n",
    "print(f\"Working Directory: {wdir}\")\n",
    "print(f\"Processed Directory: {processed_dir}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Directory: D:/lyx20/Documents/LiuYuxuan/universite/Year4/csc413/a2a-music-gen/notebooks/..\n",
      "Processed Directory: D:/lyx20/Documents/LiuYuxuan/universite/Year4/csc413/a2a-music-gen/notebooks/../data/processed/musicnet\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T00:53:24.322399Z",
     "start_time": "2024-04-13T00:53:24.287405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from data import DataLoaders\n",
    "\n",
    "# Load the dataset\n",
    "dataset = SpectrogramDataset(f'{processed_dir}/train_data')\n",
    "# split dataset\n",
    "train_size = int(0.6 * len(dataset))\n",
    "val_size = (len(dataset) - train_size) // 2\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "dataloaders = DataLoaders(\n",
    "    train=DataLoader(train_dataset, batch_size=50, shuffle=True),\n",
    "    val=DataLoader(val_dataset, batch_size=10, shuffle=True),\n",
    "    test=DataLoader(test_dataset, batch_size=10, shuffle=True),\n",
    ")\n",
    "nfreqs, nframes = dataset[0][0].shape\n",
    "nfreqs, nframes"
   ],
   "id": "30df1940045accd4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 882)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T00:53:24.472230Z",
     "start_time": "2024-04-13T00:53:24.323406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup the model\n",
    "from src.models import BaseTransformer, ModularizedTransformer\n",
    "from src.models import TransformerConfig\n",
    "from src.models.trainer import Trainer\n",
    "\n",
    "model = BaseTransformer(nfreqs)\n",
    "model"
   ],
   "id": "b2bc838c4e126f7e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseTransformer(\n",
       "  (embedding): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=200, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=200, bias=True)\n",
       "          (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=200, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T00:53:24.479223Z",
     "start_time": "2024-04-13T00:53:24.473323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "26c93da5c3c97be8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T00:54:34.934862Z",
     "start_time": "2024-04-13T00:53:24.481071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample_batch = [dataset[i][0] for i in range(1)]\n",
    "src = dataset[0][0][:, 0:10] #torch.stack(sample_batch)\n",
    "tgt = dataset[0][0][:, 1:11] #torch.stack(sample_batch)\n",
    "print(src.shape)\n",
    "src_mask = nn.Transformer.generate_square_subsequent_mask(len(src))\n",
    "tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt))\n",
    "src = model.embedding(src)\n",
    "src = model.positional_encoding(src)\n",
    "tgt = model.embedding(tgt)\n",
    "tgt = model.positional_encoding(tgt)\n",
    "\n",
    "out = model.transformer(src, tgt)\n",
    "out = model.output(out)\n",
    "# model(sample_batch.T)\n",
    "out.shape"
   ],
   "id": "4477a664fe660fca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 200])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m tgt \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39membedding(tgt)\n\u001B[0;32m      8\u001B[0m tgt \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpositional_encoding(tgt)\n\u001B[1;32m---> 10\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m out \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39moutput(out)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# model(sample_batch.T)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:208\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe feature number of src and tgt must be equal to d_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    206\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(src, mask\u001B[38;5;241m=\u001B[39msrc_mask, src_key_padding_mask\u001B[38;5;241m=\u001B[39msrc_key_padding_mask,\n\u001B[0;32m    207\u001B[0m                       is_causal\u001B[38;5;241m=\u001B[39msrc_is_causal)\n\u001B[1;32m--> 208\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:465\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    462\u001B[0m tgt_is_causal \u001B[38;5;241m=\u001B[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 465\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    466\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    467\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    468\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    469\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    473\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(output)\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:855\u001B[0m, in \u001B[0;36mTransformerDecoderLayer.forward\u001B[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    853\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x))\n\u001B[0;32m    854\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 855\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sa_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    856\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001B[0;32m    857\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:864\u001B[0m, in \u001B[0;36mTransformerDecoderLayer._sa_block\u001B[1;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001B[0m\n\u001B[0;32m    862\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sa_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor,\n\u001B[0;32m    863\u001B[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 864\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m                       \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    869\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout1(x)\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   1227\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1228\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1229\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1238\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1239\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1240\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1241\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1242\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1243\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1244\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1250\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1253\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32mC:\\MyApps\\anaconda3\\envs\\csc413\\Lib\\site-packages\\torch\\nn\\functional.py:5479\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   5476\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n\u001B[0;32m   5477\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(bsz \u001B[38;5;241m*\u001B[39m tgt_len, embed_dim)\n\u001B[1;32m-> 5479\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattn_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_proj_bias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5480\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m attn_output\u001B[38;5;241m.\u001B[39mview(tgt_len, bsz, attn_output\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m   5481\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[0;32m   5482\u001B[0m     \u001B[38;5;66;03m# squeeze the output if input was unbatched\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_accuracy(output: torch.Tensor, target: torch.Tensor) -> Tensor:\n",
    "    # Calculate accuracy by finding cos similiraty\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    return torch.mean(torch.abs(cos(output, target)))\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        weight_decay: float,\n",
    "        learning_rate: float,\n",
    "        num_epochs: int,\n",
    "        device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ) -> pd.DataFrame:\n",
    "\n",
    "        model = model.to(device)  # move model to GPU if applicable\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), lr=learning_rate, weight_decay=weight_decay\n",
    "        )\n",
    "        history = []\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            train_loss = 0.0\n",
    "            val_loss = 0.0\n",
    "\n",
    "            train_acc = 0\n",
    "            val_acc = 0\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for data, target, names in dataloaders.train:\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Predicted outputs\n",
    "                output = model(data)\n",
    "\n",
    "                # Loss and backpropagation of gradients\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "\n",
    "                # Update the parameters\n",
    "                optimizer.step()\n",
    "\n",
    "                # Track train loss by multiplying average loss by number of examples in batch\n",
    "                train_loss += loss.item() * data.size(0)\n",
    "                # check target have same shape as output\n",
    "                target = target.data.view_as(output)\n",
    "                accuracy = get_accuracy(output, target)\n",
    "                # Multiply average accuracy times the number of examples in batch\n",
    "                train_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "            # Don't need to keep track of gradients\n",
    "            with torch.no_grad():\n",
    "                # Set to evaluation mode\n",
    "                model.eval()\n",
    "\n",
    "                # Validation loop\n",
    "                for data, target, names in dataloaders.val:\n",
    "                    # Tensors to gpu\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                    # Forward pass\n",
    "                    output = model(data)\n",
    "\n",
    "                    # Validation loss\n",
    "                    loss = criterion(output, target)\n",
    "                    # Multiply average loss times the number of examples in batch\n",
    "                    val_loss += loss.item() * data.size(0)\n",
    "\n",
    "                    # check target have same shape as output\n",
    "                    target = target.data.view_as(output)\n",
    "                    accuracy = get_accuracy(output, target)\n",
    "                    # Multiply average accuracy times the number of examples\n",
    "                    val_acc += accuracy.item() * data.size(0)\n",
    "\n",
    "                # Calculate average losses\n",
    "                train_loss = train_loss / len(dataloaders.train.dataset)\n",
    "                val_loss = val_loss / len(dataloaders.val.dataset)\n",
    "\n",
    "                # Calculate average accuracy\n",
    "                train_acc = train_acc / len(dataloaders.train.dataset)\n",
    "                val_acc = val_acc / len(dataloaders.val.dataset)\n",
    "\n",
    "                print(\n",
    "                    f\"\\nEpoch: {e} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {val_loss:.4f}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"\\t \\tTraining Accuracy: {100 * train_acc:.2f}% \\tValidation Accuracy: {100 * val_acc:.2f}%\"\n",
    "                )\n",
    "                history.append([train_loss, val_loss, train_acc, val_acc])\n",
    "\n",
    "        return pd.DataFrame(\n",
    "            history, columns=[\"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"]\n",
    "        )"
   ],
   "id": "8b6a53e3833f569f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
